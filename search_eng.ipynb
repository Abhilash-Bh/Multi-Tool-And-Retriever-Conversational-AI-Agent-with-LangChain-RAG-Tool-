{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIKI WITH RAG TOOL EXECUTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updation: use 2 or more rag chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki tool \n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tavily  search TOOL \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API\"]=os.getenv(\"TAVILY_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API\")\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY environment variable is not set.\")\n",
    "\n",
    "tavily_tool = TavilySearchResults(k=5, tavily_api_key=tavily_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['HF_TOCKEN']=os.getenv(\"HF_TOCKEN\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaming\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001EEA3F73380>, search_kwargs={})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever tool \n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents, embeddings)\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create own tools bad on own created retriever\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith-search\",\"Search any information about Langsmith \") # retriever , retrievername. description\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,retriever_tool,tavily_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaming\\AppData\\Roaming\\Python\\Python312\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Template\n",
    "from langchain import hub\n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEDFBBBEC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEDFBBBEC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EEF90C1E80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EEF90C1670>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents promt and llm chains\n",
    "# you cannot chain tools in prompt templete so need to call this tool agent to chain tools with llm and prompt \n",
    "# make tools agents\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEDFBBBEC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEDFBBBEC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EEF90C1E80>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EEF90C1670>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'C:\\\\Users\\\\Gaming\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='langsmith-search', description='Search any information about Langsmith ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001EEA54EDF80>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001EEA3F73380>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001EEA54EF2E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001EEA3F73380>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')), TavilySearchResults(api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'What is Langsmith and how to use it with LangChain?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mGet started with LangSmith | ðŸ¦œï¸ðŸ› ï¸ LangSmith\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic â€”Â it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observabilityâ€‹\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmithâ€™s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic â€” score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evalsâ€‹\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineeringâ€‹\n",
      "While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaming\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'How can I use Langsmith with LangChain?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mGet started with LangSmith | ðŸ¦œï¸ðŸ› ï¸ LangSmith\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic â€”Â it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observabilityâ€‹\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmithâ€™s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic â€” score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evalsâ€‹\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineeringâ€‹\n",
      "While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaming\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mIt seems that Langsmith is a platform for building production-grade LLM applications, and it can be used with LangChain. Langsmith provides features such as observability, evaluation, and prompt engineering to help you build and optimize your LLM applications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith and how to use it with LangChain?',\n",
       " 'output': 'It seems that Langsmith is a platform for building production-grade LLM applications, and it can be used with LangChain. Langsmith provides features such as observability, evaluation, and prompt engineering to help you build and optimize your LLM applications.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith and how to use it with LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Indian cricket team'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: India national cricket team\n",
      "Summary: The India men's national cricket team, also known as Men in Blue, represents India in international cricket. It is governed by the Board of Control for Cricket in India (BCCI) and is a full member nation of \u001b[0m\u001b[32;1m\u001b[1;3mThe India national cricket team, also known as the Men in Blue, represents India in international cricket. It is governed by the Board of Control for Cricket in India (BCCI) and is a full member nation of the International Cricket Council (ICC). The team has a rich history of producing some of the best cricketers in the world, including Sachin Tendulkar, Sunil Gavaskar, Kapil Dev, and many others.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Indian cricket?',\n",
       " 'output': 'The India national cricket team, also known as the Men in Blue, represents India in international cricket. It is governed by the Board of Control for Cricket in India (BCCI) and is a full member nation of the International Cricket Council (ICC). The team has a rich history of producing some of the best cricketers in the world, including Sachin Tendulkar, Sunil Gavaskar, Kapil Dev, and many others.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Indian cricket?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'AI news todays'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Perplexity AI\n",
      "Summary: Perplexity AI, Inc., or simply Perplexity, is an American privately held software company offering a web search engine that processes user queries and synthesizes responses. It uses large language models and incorporates \u001b[0m\u001b[32;1m\u001b[1;3mIt seems like the tool call yielded some information about Perplexity AI, but it doesn't seem directly related to \"AI news today\". Let me try to use another tool to get more relevant information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about todays ai news',\n",
       " 'output': 'It seems like the tool call yielded some information about Perplexity AI, but it doesn\\'t seem directly related to \"AI news today\". Let me try to use another tool to get more relevant information.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about todays ai news\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': '2022 Wimbledon Championships'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: 2022 Wimbledon Championships\n",
      "Summary: The 2022 Wimbledon Championships was a Grand Slam tier tennis tournament that took place at the All England Lawn Tennis and Croquet Club in Wimbledon, London, United Kingdom. Novak Djokovic successfully def\u001b[0m\u001b[32;1m\u001b[1;3mNovak Djokovic successfully defended his title, winning his 21st Grand Slam singles title and his 7th Wimbledon title, by defeating Nick Kyrgios in the final with a score of 4-6, 6-3, 6-4, 7-6(7-3).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What happened at the last wimbledon',\n",
       " 'output': 'Novak Djokovic successfully defended his title, winning his 21st Grand Slam singles title and his 7th Wimbledon title, by defeating Nick Kyrgios in the final with a score of 4-6, 6-3, 6-4, 7-6(7-3).'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What happened at the last wimbledon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'roadmap of data science search in tavilly'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m[{'title': 'Tavily - The Web Access Layer for AI Agents', 'url': 'https://tavily.com/', 'content': \"If you have any questions, please do not hesitate to reach to our team.\\n\\nContact Us\\n\\nWhat is Tavily Search API?\\n\\nTavily Search API is a specialized search engine designed for Large Language Models (LLMs) and AI agents. It provides real-time, accurate, and unbiased information, enabling AI applications to retrieve and process data efficiently. Tavily is built with AI developers in mind, simplifying the process of integrating dynamic web information into AI-driven solutions. [...] Search is essential for AI assistants. At CopilotKit, we build deeply-integrated AI assistants where search is a critical feature. Tavily's Search API fills this gap, providing accurate, up-to-date information that transforms AI copilots into invaluable productivity tools for our customers from day one.\\n\\nTestimonial\\n\\nAtai Barkai\\n\\nCo-Founder of CopilotKit\\n\\nquotes [...] How is Tavily Search API different from other APIs?\\n\\nUnlike Bing, Google and SerpAPI, Tavily Search API reviews multiple sources to find the most relevant content from each source, delivering concise, ready-to-use information optimized for LLM context. This focus on RAG and LLMs ensures your AI applications access only the highest-quality data. Tavily Search API is also more affordable and flexible.\\n\\nWhat are the key advantages of using Tavily Search API?\", 'score': 0.5792344}, {'title': 'How to Give Your AI Agents Real-Time Search Superpowers with ...', 'url': 'https://medium.com/@abdibrokhim/how-to-give-your-ai-agents-real-time-search-superpowers-with-tavily-c21fd8aa8cb7', 'content': '3. AI calls `searchWeb(â€œBitcoin price news todayâ€)`\\n\\n4. Tavily returns real-time results\\n\\n5. AI synthesizes the information into a natural response\\n\\nAll automatic! ðŸ¤¯\\n\\nðŸ—žï¸ Specialized functions for different use cases:\\n\\n// General web search\\n\\nawait tavilyService.searchGeneral(query, maxResults)\\n\\n// Current news\\n\\nawait tavilyService.searchNews(query, maxResults, \\'day\\')\\n\\n// Quick answers\\n\\nawait tavilyService.getQuickAnswer(query)\\nEach optimized for different types of information retrieval! ðŸ“Š [...] Published Time: 2025-06-24T13:41:06.370Z\\n\\nHow to Give Your AI Agents Real-Time Search Superpowers with Tavily | by Ibrohim Abdivokhidov | Jun, 2025 | Medium\\n\\n===============\\n\\nSitemap\\n\\nOpen in app\\n\\nSign up\\n\\nSign in\\n\\n {\\n\\n const response = await fetch(\\' {\\n\\n method: \\'POST\\',\\n\\n headers: {\\n\\n \\'Authorization\\': `Bearer ${this.apiKey}`,\\n\\n \\'Content-Type\\': \\'application/json\\'\\n\\n },\\n\\n body: JSON.stringify(searchPayload)\\n\\n });\\n\\n return response.json();\\n\\n } [...] }\\nðŸŽ¯ Function calling integration is where it gets interesting. When the AI decides it needs current information, it automatically calls the search function:\\n\\ncase \"searchWeb\":\\n\\n const searchResponse = await tavilyService.search({\\n\\n query: args.query,\\n\\n topic: args.topic || \\'general\\',\\n\\n max_results: args.max_results || 5\\n\\n });\\n\\n return JSON.stringify(searchResponse);\\nðŸ“ˆ The AI agent conversation flow:\\n\\n1. User asks: â€œWhatâ€™s happening with Bitcoin today?â€\\n\\n2. AI recognizes it needs current data', 'score': 0.5663309}, {'title': 'DATA SCIENTIST ROADMAP : r/datascience - Reddit', 'url': 'https://www.reddit.com/r/datascience/comments/10g4tqr/data_scientist_roadmap/', 'content': \"DATA SCIENTIST ROADMAP : r/datascience\\n\\n===============\\nSkip to main contentDATA SCIENTIST ROADMAP : r/datascience\\n\\nOpen menu Open navigation and descriptive statistics (I know the concepts but I am not a genius).Having said all that, where do you think is the best place to start? mathematics, statistics or python? Btw I'm currently in my 3rd semester of systems engineering.\\n\\n Read more \\n\\n Archived post. New comments cannot be posted and votes cannot be cast. \\n\\n Share\", 'score': 0.4233394}, {'title': 'Tavily Search - LangChain', 'url': 'https://python.langchain.com/docs/integrations/tools/tavily_search/', 'content': 'For a comprehensive overview of the available parameters, refer to the Tavily Search API documentation\\n\\n`from langchain_tavily import TavilySearchtool = TavilySearch(    max_results=5,    topic=\"general\",    # include_answer=False,    # include_raw_content=False,    # include_images=False,    # include_image_descriptions=False,    # search_depth=\"basic\",    # time_range=\"day\",    # include_domains=None,    # exclude_domains=None)` [...] Here we show how to instantiate an instance of the Tavily search tool. The tool accepts various parameters to customize the search. After instantiation we invoke the tool with a simple query. This tool allows you to complete search queries using Tavily\\'s Search API endpoint.\\n\\nInstantiation The tool accepts various parameters during instantiation: [...] `from langchain_tavily import TavilySearchfrom langgraph.prebuilt import create_react_agent# Initialize Tavily Search Tooltavily_search_tool = TavilySearch(    max_results=5,    topic=\"general\",)agent = create_react_agent(llm, [tavily_search_tool])user_input = \"What nation hosted the Euro 2024? Include only wikipedia sources.\"for step in agent.stream(    {\"messages\": user_input},    stream_mode=\"values\",):    step[\"messages\"][-1].pretty_print()`\\n\\nAPI Reference:create_react_agent', 'score': 0.40161797}, {'title': '[PDF] Data Science Roadmap | Code with Mosh', 'url': 'https://cdn.codewithmosh.com/image/upload/v1721942360/guides/data-science-roadmap.pdf', 'content': 'and Linked Lists â€¢ Stacks and Queues â€¢ Hash Tables â€¢ Trees and Graphs â€¢ Binary trees â€¢ AVL trees â€¢ Heaps â€¢ Tries â€¢ Graphs â€¢ Sorting Algorithms â€¢ Bubble sort â€¢ Selection sort â€¢ Insertion sort â€¢ Merge sort Copyright 2024 Code with Mosh codewithmosh.com Data Science Roadmap 10 â€¢ Quick sort â€¢ Counting sort â€¢ Bucket sort â€¢ Searching algorithms â€¢ Linear search â€¢ Binary search â€¢ Ternary search â€¢ Jump search â€¢ Exponential search â€¢ String Manipulation Algorithms â€¢ Reversing a string â€¢ Reversing words â€¢ [...] unlock your potential in the world of coding!  Data Science Roadmap 3 Table of Content Introduction 4 Target Audience 4 Resources 4 Roadmap Overview 5 Python 6 Version Control (Git) 8 Data Structures & Algorithms 9 SQL 11 Mathematics and Statistics 13 Data Collection and Visualization 15 Machine Learning Fundamentals 16 Deep Learning 18 Specialization 19 Big Data (Optional) 20 Copyright 2024 Code with Mosh codewithmosh.com Data Science Roadmap 4 Introduction This guide is designed to help you [...] Recurrent Neural Networks (RNNs) â€¢ Deep Learning Frameworks â€¢ Tools: TensorFlow, PyTorch, Keras Copyright 2024 Code with Mosh codewithmosh.com Data Science Roadmap 19 Specialization Specializing in a specific area of data science allows you to develop expertise and stand out in the field. Two popular tracks are Natural Language Processing (NLP) and Computer Vision. Estimated Time: 2-3 months Essential Concepts â€¢ Natural Language Processing (NLP) â€¢ Text preprocessing (tokenization, stemming,', 'score': 0.38588423}]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the results, it seems that Tavily Search API is a key player in the search for data science-related information. The results suggest that Tavily is a search engine designed for Large Language Models (LLMs) and AI agents, providing real-time, accurate, and unbiased information.\n",
      "\n",
      "One of the most relevant results is the Medium article \"How to Give Your AI Agents Real-Time Search Superpowers with Tavily\" which highlights the capabilities of Tavily Search API in providing real-time search results for AI agents.\n",
      "\n",
      "Another relevant result is the LangChain documentation for Tavily Search, which shows how to integrate Tavily Search API with LangChain to enable real-time search capabilities for AI agents.\n",
      "\n",
      "The results also suggest that there are various tools and frameworks available for data science, such as Python, SQL, and Deep Learning frameworks like TensorFlow, PyTorch, and Keras.\n",
      "\n",
      "As for the roadmap for data science search in Tavily, it seems that Tavily is continuously improving its search capabilities and expanding its features to better serve the needs of data scientists. The results suggest that Tavily is a key player in the search for data science-related information and is well-positioned to continue to provide valuable insights and knowledge to data scientists.\n",
      "\n",
      "Here is a possible roadmap for data science search in Tavily:\n",
      "\n",
      "1. Improved search engine: Tavily will continue to improve its search engine to provide more accurate and relevant results for data science-related queries.\n",
      "2. Integration with AI agents: Tavily will continue to integrate its search capabilities with AI agents to enable real-time search and retrieval of data science-related information.\n",
      "3. Expanded features: Tavily will continue to expand its features to include more advanced search capabilities, such as natural language processing and machine learning.\n",
      "4. Increased data sets: Tavily will continue to provide access to a wider range of data sets and sources to enable data scientists to access more comprehensive and accurate information.\n",
      "5. Improved user interface: Tavily will continue to improve its user interface to make it easier for data scientists to access and use its search capabilities.\n",
      "\n",
      "Overall, the results suggest that Tavily is well-positioned to continue to provide valuable insights and knowledge to data scientists and is likely to remain a key player in the search for data science-related information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'after complete data analytics what is the roadmap of data science search in tavilly',\n",
       " 'output': 'Based on the results, it seems that Tavily Search API is a key player in the search for data science-related information. The results suggest that Tavily is a search engine designed for Large Language Models (LLMs) and AI agents, providing real-time, accurate, and unbiased information.\\n\\nOne of the most relevant results is the Medium article \"How to Give Your AI Agents Real-Time Search Superpowers with Tavily\" which highlights the capabilities of Tavily Search API in providing real-time search results for AI agents.\\n\\nAnother relevant result is the LangChain documentation for Tavily Search, which shows how to integrate Tavily Search API with LangChain to enable real-time search capabilities for AI agents.\\n\\nThe results also suggest that there are various tools and frameworks available for data science, such as Python, SQL, and Deep Learning frameworks like TensorFlow, PyTorch, and Keras.\\n\\nAs for the roadmap for data science search in Tavily, it seems that Tavily is continuously improving its search capabilities and expanding its features to better serve the needs of data scientists. The results suggest that Tavily is a key player in the search for data science-related information and is well-positioned to continue to provide valuable insights and knowledge to data scientists.\\n\\nHere is a possible roadmap for data science search in Tavily:\\n\\n1. Improved search engine: Tavily will continue to improve its search engine to provide more accurate and relevant results for data science-related queries.\\n2. Integration with AI agents: Tavily will continue to integrate its search capabilities with AI agents to enable real-time search and retrieval of data science-related information.\\n3. Expanded features: Tavily will continue to expand its features to include more advanced search capabilities, such as natural language processing and machine learning.\\n4. Increased data sets: Tavily will continue to provide access to a wider range of data sets and sources to enable data scientists to access more comprehensive and accurate information.\\n5. Improved user interface: Tavily will continue to improve its user interface to make it easier for data scientists to access and use its search capabilities.\\n\\nOverall, the results suggest that Tavily is well-positioned to continue to provide valuable insights and knowledge to data scientists and is likely to remain a key player in the search for data science-related information.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"after complete data analytics what is the roadmap of data science search in tavilly\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
